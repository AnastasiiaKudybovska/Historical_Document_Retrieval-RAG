{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-12-09T13:32:35.840231Z",
          "iopub.status.busy": "2024-12-09T13:32:35.839756Z",
          "iopub.status.idle": "2024-12-09T13:36:05.795358Z",
          "shell.execute_reply": "2024-12-09T13:36:05.794189Z",
          "shell.execute_reply.started": "2024-12-09T13:32:35.840163Z"
        },
        "id": "q9VUWQvFC0XP",
        "outputId": "e6a9ee90-6664-43d0-e8d2-7675d9c27b6d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf 24.10.1 requires cubinlinker, which is not installed.\n",
            "cudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
            "cudf 24.10.1 requires libcudf==24.10.*, which is not installed.\n",
            "cudf 24.10.1 requires ptxcompiler, which is not installed.\n",
            "cuml 24.10.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
            "cuml 24.10.0 requires cuvs==24.10.*, which is not installed.\n",
            "cuml 24.10.0 requires nvidia-cublas, which is not installed.\n",
            "cuml 24.10.0 requires nvidia-cufft, which is not installed.\n",
            "cuml 24.10.0 requires nvidia-curand, which is not installed.\n",
            "cuml 24.10.0 requires nvidia-cusolver, which is not installed.\n",
            "cuml 24.10.0 requires nvidia-cusparse, which is not installed.\n",
            "dask-cudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
            "bigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\n",
            "bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
            "bigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.3 which is incompatible.\n",
            "cudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\n",
            "cudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "dask-cudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.10.1 which is incompatible.\n",
            "distributed 2024.9.0 requires dask==2024.9.0, but you have dask 2024.11.2 which is incompatible.\n",
            "gcsfs 2024.9.0.post1 requires fsspec==2024.9.0, but you have fsspec 2024.3.1 which is incompatible.\n",
            "rapids-dask-dependency 24.10.0a0 requires dask==2024.9.0, but you have dask 2024.11.2 which is incompatible.\n",
            "rapids-dask-dependency 24.10.0a0 requires dask-expr==1.1.14, but you have dask-expr 1.1.19 which is incompatible.\n",
            "s3fs 2024.9.0 requires fsspec==2024.9.0.*, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.8.0 requires huggingface-hub>=0.25.1, but you have huggingface-hub 0.23.5 which is incompatible.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-index-llms-huggingface 0.4.0 requires huggingface-hub<0.24.0,>=0.23.0, but you have huggingface-hub 0.26.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber -q\n",
        "!pip install -U FlagEmbedding -q\n",
        "!pip install gradio --q\n",
        "!pip install -q llama-index\n",
        "!pip install -q llama-core\n",
        "!pip install -q llama-index-llms-huggingface\n",
        "!pip install -q llama-index-embeddings-huggingface\n",
        "!pip install -q llama-index-embeddings-huggingface-api\n",
        "!pip install groq -q\n",
        "!pip install rank_bm25 -q\n",
        "!pip install --upgrade huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:36:05.797692Z",
          "iopub.status.busy": "2024-12-09T13:36:05.797358Z",
          "iopub.status.idle": "2024-12-09T13:36:26.849494Z",
          "shell.execute_reply": "2024-12-09T13:36:26.848813Z",
          "shell.execute_reply.started": "2024-12-09T13:36:05.797661Z"
        },
        "id": "CYGqbwzHDcBF",
        "outputId": "b1d0c998-9ce6-43df-d70f-1d31ba9652d8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pdfplumber\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from groq import Groq\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:36:26.851024Z",
          "iopub.status.busy": "2024-12-09T13:36:26.850387Z",
          "iopub.status.idle": "2024-12-09T13:36:26.856307Z",
          "shell.execute_reply": "2024-12-09T13:36:26.855478Z",
          "shell.execute_reply.started": "2024-12-09T13:36:26.850996Z"
        },
        "id": "TZaEWK8UDwCE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def extract_text_without_header(pdf_path):\n",
        "    text = \"\"\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_number, page in enumerate(pdf.pages, start=1):\n",
        "            page_text = page.extract_text()\n",
        "\n",
        "            if page_text:\n",
        "                page_text = page_text.split(\"\\n\", 1)[-1] # remove header\n",
        "\n",
        "            text += page_text + \"\\n\" if page_text else \"\\n\"\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def split_text(text):\n",
        "    sections = text.split(\"§\")\n",
        "    return sections[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:36:26.858292Z",
          "iopub.status.busy": "2024-12-09T13:36:26.858034Z",
          "iopub.status.idle": "2024-12-09T13:36:26.865883Z",
          "shell.execute_reply": "2024-12-09T13:36:26.865128Z",
          "shell.execute_reply.started": "2024-12-09T13:36:26.858268Z"
        },
        "id": "I5S3MMCnE8T-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pdf_file_path = \"/kaggle/input/history-book/Istoriia-ukrainy-7-klas-Sorochynska-2020.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:36:26.867101Z",
          "iopub.status.busy": "2024-12-09T13:36:26.866874Z",
          "iopub.status.idle": "2024-12-09T13:37:03.683290Z",
          "shell.execute_reply": "2024-12-09T13:37:03.682605Z",
          "shell.execute_reply.started": "2024-12-09T13:36:26.867078Z"
        },
        "id": "B-sTfLjENnQg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pdf_text = extract_text_without_header(pdf_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:37:03.685122Z",
          "iopub.status.busy": "2024-12-09T13:37:03.684478Z",
          "iopub.status.idle": "2024-12-09T13:37:03.691704Z",
          "shell.execute_reply": "2024-12-09T13:37:03.690797Z",
          "shell.execute_reply.started": "2024-12-09T13:37:03.685083Z"
        },
        "id": "_R0qswoakJ44",
        "outputId": "fbeea6f9-6081-4ca4-95bf-47c0f520fdc5",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sections = split_text(pdf_text)\n",
        "len(sections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:37:03.693081Z",
          "iopub.status.busy": "2024-12-09T13:37:03.692848Z",
          "iopub.status.idle": "2024-12-09T13:37:03.702216Z",
          "shell.execute_reply": "2024-12-09T13:37:03.701276Z",
          "shell.execute_reply.started": "2024-12-09T13:37:03.693058Z"
        },
        "id": "Au40Iz3th6OQ",
        "outputId": "347ca7f8-e5e8-4811-99aa-92a5854cd444",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Виникнення і становлення Русі-України\\nbohdan-books.com/\\nПовторимо вивчене з історії України у 6 класі upload/data_files/\\ntmp_catalog/\\nupovt.pdf\\n§ 1. ВСТУП ДО СЕРЕДНЬОВІЧНОЇ ІСТОРІЇ УКРАЇНИ\\n1. Історія України як наука і навчальний предмет\\nУ 7 класі ви серед інших навчальних предметів продовжите вивче'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf_text[:300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:37:03.703474Z",
          "iopub.status.busy": "2024-12-09T13:37:03.703184Z",
          "iopub.status.idle": "2024-12-09T13:37:03.714178Z",
          "shell.execute_reply": "2024-12-09T13:37:03.713444Z",
          "shell.execute_reply.started": "2024-12-09T13:37:03.703447Z"
        },
        "id": "HFibhRL6y-NN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def clean_sections(sections):\n",
        "    cleaned_sections = []\n",
        "    for section in sections:\n",
        "        section = section.replace(\"bohdan-books.com/\", \"\")\n",
        "        section = section.replace(\"upload/data_files/\", \"\")\n",
        "        section = section.replace(\"tmp_catalog/\", \"\")\n",
        "        pattern = r'\\b\\w+\\.pdf\\b'\n",
        "\n",
        "        cleaned_section = re.sub(r'(\\w+)\\s*-\\s*\\n\\s*(\\w+)', r'\\1\\2', section)\n",
        "        cleaned_section = re.sub(r'(\\S)\\s*\\n\\s*(\\S)', r'\\1 \\2', cleaned_section)\n",
        "        cleaned_section = re.sub(pattern, '', cleaned_section)\n",
        "        cleaned_sections.append(cleaned_section)\n",
        "    return cleaned_sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:37:03.715972Z",
          "iopub.status.busy": "2024-12-09T13:37:03.715198Z",
          "iopub.status.idle": "2024-12-09T13:37:03.806007Z",
          "shell.execute_reply": "2024-12-09T13:37:03.805421Z",
          "shell.execute_reply.started": "2024-12-09T13:37:03.715933Z"
        },
        "id": "oQdSmy0zPKV2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "cleaned_sections = clean_sections(sections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:37:03.808495Z",
          "iopub.status.busy": "2024-12-09T13:37:03.808218Z",
          "iopub.status.idle": "2024-12-09T13:37:03.812791Z",
          "shell.execute_reply": "2024-12-09T13:37:03.811923Z",
          "shell.execute_reply.started": "2024-12-09T13:37:03.808471Z"
        },
        "id": "g54lY9DoPRXu",
        "outputId": "869bc0bf-4a4e-4fee-9bb9-cc11187cbbd8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 4. КНЯЗЮВАННЯ ІГОРЯ ТА ОЛЬГИ  1. Правління князя Ігоря Наступником Олега на київському престолі став син Рюрика — Ігор. Князь Ігор (? — 945) продовжував справу свого попередника, згуртовуючи східних слов’ян у єдину державу. Розпочав своє правління боротьбою з деревлянами та уличами, які вийшли з покори Києву. Ігор наклав на них значно більшу данину, ніж раніше. У відповідь уличі залишили Середнє Подніпров’я і переселилися в межиріччя Дністра і Південного Бугу. У 915 р. біля кордонів Київської д\n"
          ]
        }
      ],
      "source": [
        "print(cleaned_sections[3][:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:37:03.814013Z",
          "iopub.status.busy": "2024-12-09T13:37:03.813774Z",
          "iopub.status.idle": "2024-12-09T13:37:03.820886Z",
          "shell.execute_reply": "2024-12-09T13:37:03.820204Z",
          "shell.execute_reply.started": "2024-12-09T13:37:03.813990Z"
        },
        "id": "Gnpfqw9pNvcr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def save_as_txt(content, filename, directory):\n",
        "    with open(os.path.join(directory, f\"{filename}.txt\"), 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "\n",
        "def save_sections_as_files(sections, output_dir, file_format='txt'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for idx, section_content in enumerate(sections, start=1):\n",
        "        filename = f\"Section_{section_content['paragraph_number']}_{section_content['point_number']}\"\n",
        "        if file_format.lower() == 'txt':\n",
        "            save_as_txt(section_content['text'], filename, output_dir)\n",
        "\n",
        "    print(f\"Sections saved as {file_format.upper()} files in: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:37:03.822107Z",
          "iopub.status.busy": "2024-12-09T13:37:03.821846Z",
          "iopub.status.idle": "2024-12-09T13:37:03.829541Z",
          "shell.execute_reply": "2024-12-09T13:37:03.828765Z",
          "shell.execute_reply.started": "2024-12-09T13:37:03.822079Z"
        },
        "id": "sl5JSJQ5pa3P",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def split_to_points(sections):\n",
        "    points_sections = []\n",
        "\n",
        "    for sec in cleaned_sections:\n",
        "        idx = sec.find(\"ВИСНОВКИ\")\n",
        "        sec_no_q = sec[:idx]\n",
        "        points = re.split(r'(?=\\b(?:1[0-9]|2[0-3]|[1-9])[-]?(?:[0-9]+)?\\.\\s)', sec_no_q)\n",
        "        title = points[0] + points[1]\n",
        "        dot_idx = title.find(\".\")\n",
        "        title_number = title[1:dot_idx]\n",
        "        for i, point in enumerate(points[2:], start=1):\n",
        "          point = title + point\n",
        "          points_sections.append({\"text\": point, \"paragraph_number\": title_number, \"point_number\": i})\n",
        "\n",
        "    return points_sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:37:03.830656Z",
          "iopub.status.busy": "2024-12-09T13:37:03.830371Z",
          "iopub.status.idle": "2024-12-09T13:37:03.856383Z",
          "shell.execute_reply": "2024-12-09T13:37:03.855462Z",
          "shell.execute_reply.started": "2024-12-09T13:37:03.830619Z"
        },
        "id": "k7a4wcVrp9Qn",
        "outputId": "9ad8bc08-335b-40cf-f6f0-0faeb5dfbd4d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sections saved as TXT files in: /kaggle/working/point_sections\n"
          ]
        }
      ],
      "source": [
        "points_sections = split_to_points(cleaned_sections)\n",
        "save_sections_as_files(points_sections, \"/kaggle/working/point_sections\", file_format='txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFG4pPRse6K7"
      },
      "source": [
        "# Semantic Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:37:03.857655Z",
          "iopub.status.busy": "2024-12-09T13:37:03.857366Z",
          "iopub.status.idle": "2024-12-09T13:37:03.862169Z",
          "shell.execute_reply": "2024-12-09T13:37:03.861295Z",
          "shell.execute_reply.started": "2024-12-09T13:37:03.857631Z"
        },
        "id": "RlVyK9vJQPcm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_index_from_documents(directory_path):\n",
        "    embed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large\")\n",
        "    reader = SimpleDirectoryReader(input_dir=directory_path)\n",
        "    documents = reader.load_data()\n",
        "\n",
        "    Settings.embed_model = embed_model\n",
        "    Settings.llm = None\n",
        "\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "      documents,\n",
        "    )\n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:37:03.863875Z",
          "iopub.status.busy": "2024-12-09T13:37:03.863271Z",
          "iopub.status.idle": "2024-12-09T13:37:03.872314Z",
          "shell.execute_reply": "2024-12-09T13:37:03.871607Z",
          "shell.execute_reply.started": "2024-12-09T13:37:03.863837Z"
        },
        "id": "683aDhmp46Mz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def extract_numbers(filename):\n",
        "    match = re.match(r'Section_(\\d+[-\\d]*)(?:[-_](\\d+))?.txt', filename)\n",
        "\n",
        "    if match:\n",
        "        paragraph_number = match.group(1)  # Paragraph number\n",
        "        point_number = match.group(2) if match.group(2) else None  # Point number (if present)\n",
        "        return paragraph_number, point_number\n",
        "    else:\n",
        "        print(filename)\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:37:03.873560Z",
          "iopub.status.busy": "2024-12-09T13:37:03.873279Z",
          "iopub.status.idle": "2024-12-09T13:37:03.884076Z",
          "shell.execute_reply": "2024-12-09T13:37:03.883467Z",
          "shell.execute_reply.started": "2024-12-09T13:37:03.873534Z"
        },
        "id": "TfsoSdNnQiZH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def semantic_search(query, index, top_n=3):\n",
        "    retriever = index.as_retriever(similarity_top_k=top_n)\n",
        "    response = retriever.retrieve(query)\n",
        "    retrieved_documents = []\n",
        "    for node in response:\n",
        "        file_name = node.node.metadata[\"file_name\"]\n",
        "        paragraph_number, point_number = extract_numbers(file_name)\n",
        "\n",
        "        retrieved_documents.append({\"text\": node.node.text,\n",
        "                                    \"score\": node.score,\n",
        "                                    \"paragraph_number\": paragraph_number,\n",
        "                                    \"point_number\": point_number\n",
        "                                   })\n",
        "\n",
        "    return retrieved_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y65nmasjYNKf"
      },
      "source": [
        "# BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:37:03.885762Z",
          "iopub.status.busy": "2024-12-09T13:37:03.885137Z",
          "iopub.status.idle": "2024-12-09T13:37:03.892521Z",
          "shell.execute_reply": "2024-12-09T13:37:03.891675Z",
          "shell.execute_reply.started": "2024-12-09T13:37:03.885725Z"
        },
        "id": "JGNGyApGRMOq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def bm25_search(query, points_sections, top_n=3):\n",
        "    point_sections_text = [point[\"text\"] for point in points_sections]\n",
        "    points_sections_lowercase = list(map(str.lower, point_sections_text))\n",
        "    corpus = [word_tokenize(doc) for doc in points_sections_lowercase]\n",
        "\n",
        "    bm25 = BM25Okapi(corpus)\n",
        "\n",
        "    query_tokens = word_tokenize(query.lower())\n",
        "\n",
        "    scores = bm25.get_scores(query_tokens)\n",
        "\n",
        "    top_indices = np.argsort(scores)[-top_n:][::-1]\n",
        "\n",
        "    top_docs = [{\"text\": points_sections[i]['text'],\n",
        "                 \"score\": scores[i],\n",
        "                 \"paragraph_number\": points_sections[i][\"paragraph_number\"],\n",
        "                \"point_number\": points_sections[i][\"point_number\"]\n",
        "                }\n",
        "                for i in top_indices]\n",
        "\n",
        "    return top_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgLT6RFPS7O3"
      },
      "source": [
        "# Reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c8b9e73981804bd29dee6176ff4d1c96",
            "b826f343257d498c86132ac516389139",
            "71f3224e00d54e7bb1bf6b3f86c83fe0",
            "9b85d64efc6f4f25a30750fe38d063b9",
            "17a0ec9c95254b97aa7f4a0ad2bfe39d",
            "77afa091e6854b22ad3189ef2f390db4"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-09T13:37:03.893726Z",
          "iopub.status.busy": "2024-12-09T13:37:03.893483Z",
          "iopub.status.idle": "2024-12-09T13:38:00.404223Z",
          "shell.execute_reply": "2024-12-09T13:38:00.403271Z",
          "shell.execute_reply.started": "2024-12-09T13:37:03.893703Z"
        },
        "id": "Gz38CKwaS6vA",
        "outputId": "7e2b0093-2b28-449d-c16a-eccbc0097791",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8b9e73981804bd29dee6176ff4d1c96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b826f343257d498c86132ac516389139",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71f3224e00d54e7bb1bf6b3f86c83fe0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b85d64efc6f4f25a30750fe38d063b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17a0ec9c95254b97aa7f4a0ad2bfe39d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/795 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77afa091e6854b22ad3189ef2f390db4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "rom FlagEmbedding import FlagReranker\n",
        "reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)\n",
        "\n",
        "def use_reranker(query, documents, top_k=3):\n",
        "    pairs = [[query, doc[\"text\"]] for doc in documents]\n",
        "    scores = reranker.compute_score(pairs, normalize=True)\n",
        "    top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "    top_docs = [documents[i] for i in top_indices]\n",
        "    return top_docsf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqq1wIbKewjE"
      },
      "source": [
        "# LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:38:00.405932Z",
          "iopub.status.busy": "2024-12-09T13:38:00.405566Z",
          "iopub.status.idle": "2024-12-09T13:38:00.412286Z",
          "shell.execute_reply": "2024-12-09T13:38:00.411230Z",
          "shell.execute_reply.started": "2024-12-09T13:38:00.405893Z"
        },
        "id": "tJWowehKevD2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_context(docs):\n",
        "  context = ''\n",
        "  for i, doc_info in enumerate(docs, start=1):\n",
        "      context += f\"Документ {i}: \" + doc_info['text'] + \"\\n\"\n",
        "  return context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "2aede6921f4843588fde7995c4f4150b",
            "0ade34eb8a014121bcaf6d26d4a1c32d",
            "a733ed343d34435e9cc305b12fbbacc8",
            "d3507b38a6ed43ba90f6e250a3846642",
            "144ae119fdfc47e6a765fed25ba5a7cc",
            "24703a5cda32406eb681fdbba28f93c1",
            "9f7c12742bf34877b4bf56bc44bb3f5e",
            "8b3853bfa17b451f99a01945275ad4d2",
            "36532cab8149401b8c4dd021858ba111",
            "be044600daa4465fa80ac352c94c6393",
            "a759b010cba24f70960727516c7af768",
            "234a1399446c4cdd9ad19ded4e49439e",
            "2d52fb6a5caa4bc89d05ecd753273118",
            "28636820d2e04846a361aee8841cdfbc",
            "6380f62b838a47a99eabf32c2f08b4f3",
            "91fafeb5a1db4ca187d81dc357d989b9",
            "27501d5b687d4dda9e0eba429c4f8051",
            "4532e7b6b6e64434acbbce4c0c0eb53d",
            "f1f2bcdefd474bc298ddaee2d3747e1e",
            "367b530a15e94eb7ab7f7c6c403b965c",
            "48e79a459c90455ab7b622db7b0a72f5",
            "12ab88d7f79543599975a7e44728a762",
            "9b490f4b95944f169bceff9a4d888d3b",
            "2b43ecfc65f14c1bb3a5a7bf7b4f9210",
            "31b0d6d711204ca7b06a5c1d1ed3eb18",
            "4740c6cf1f72467598530b9d2c665c32",
            "1f857a19081146b68c9225b1f1b05d22",
            "7f0ea00fdca9490d949360e6f22f5ec9",
            "7491f67aeb0e464ebedf04fbe63ccf29",
            "05a1b88d168d4068856e65a606141be4",
            "4c5de2aaa1304a7eb385d3f8e23975d9",
            "bf2133bcea264649a9352ee89be5087c",
            "8bacb2ca6ffb4c06aa49c0b8aa3e1209",
            "f2a643cc0e2a4bad90f4f7282249ddc8",
            "9b16eb80ee7e470c875221824a3ead8a",
            "0167bf5fbdf9482a900f8aaa5b629e0b"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-09T13:38:00.414185Z",
          "iopub.status.busy": "2024-12-09T13:38:00.413781Z",
          "iopub.status.idle": "2024-12-09T13:38:51.301445Z",
          "shell.execute_reply": "2024-12-09T13:38:51.300458Z",
          "shell.execute_reply.started": "2024-12-09T13:38:00.414139Z"
        },
        "id": "ceeZQVPlV23c",
        "outputId": "28763f80-d96c-49ea-e30b-49a9b1f7540d",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2aede6921f4843588fde7995c4f4150b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ade34eb8a014121bcaf6d26d4a1c32d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/160k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a733ed343d34435e9cc305b12fbbacc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3507b38a6ed43ba90f6e250a3846642",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "144ae119fdfc47e6a765fed25ba5a7cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24703a5cda32406eb681fdbba28f93c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f7c12742bf34877b4bf56bc44bb3f5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b3853bfa17b451f99a01945275ad4d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36532cab8149401b8c4dd021858ba111",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be044600daa4465fa80ac352c94c6393",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM is explicitly disabled. Using MockLLM.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a759b010cba24f70960727516c7af768",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "234a1399446c4cdd9ad19ded4e49439e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d52fb6a5caa4bc89d05ecd753273118",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28636820d2e04846a361aee8841cdfbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6380f62b838a47a99eabf32c2f08b4f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91fafeb5a1db4ca187d81dc357d989b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27501d5b687d4dda9e0eba429c4f8051",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4532e7b6b6e64434acbbce4c0c0eb53d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1f2bcdefd474bc298ddaee2d3747e1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "367b530a15e94eb7ab7f7c6c403b965c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48e79a459c90455ab7b622db7b0a72f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12ab88d7f79543599975a7e44728a762",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b490f4b95944f169bceff9a4d888d3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b43ecfc65f14c1bb3a5a7bf7b4f9210",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31b0d6d711204ca7b06a5c1d1ed3eb18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4740c6cf1f72467598530b9d2c665c32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f857a19081146b68c9225b1f1b05d22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f0ea00fdca9490d949360e6f22f5ec9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7491f67aeb0e464ebedf04fbe63ccf29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05a1b88d168d4068856e65a606141be4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c5de2aaa1304a7eb385d3f8e23975d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf2133bcea264649a9352ee89be5087c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bacb2ca6ffb4c06aa49c0b8aa3e1209",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2a643cc0e2a4bad90f4f7282249ddc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b16eb80ee7e470c875221824a3ead8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0167bf5fbdf9482a900f8aaa5b629e0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "directory_path = \"/kaggle/working/point_sections\"\n",
        "index = create_index_from_documents(directory_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:38:51.303570Z",
          "iopub.status.busy": "2024-12-09T13:38:51.302780Z",
          "iopub.status.idle": "2024-12-09T13:38:51.311177Z",
          "shell.execute_reply": "2024-12-09T13:38:51.310482Z",
          "shell.execute_reply.started": "2024-12-09T13:38:51.303532Z"
        },
        "id": "iPGd5npvzmlh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def perform_rag(query, index, points_sections, method='semantic', top_n=3, rerank=False):\n",
        "    if method == 'bm25':\n",
        "        print(\"Performing BM25 Search...\")\n",
        "        top_docs = bm25_search(query, points_sections, top_n)\n",
        "    elif method == 'semantic':\n",
        "        print(\"Performing Semantic Search...\")\n",
        "        top_docs = semantic_search(query, index, top_n)\n",
        "    else:\n",
        "        print(\"No search...\")\n",
        "        return []\n",
        "    if rerank:\n",
        "        print(\"\\nResults with reranker:\")\n",
        "        top_docs = use_reranker(query, top_docs, top_k=top_n)\n",
        "    return top_docs\n",
        "\n",
        "def perform_llm(query, use_context, groq_api_key, top_docs=None, system_prompt=\"\", temperature=0.7):\n",
        "    if use_context == 1 and top_docs:\n",
        "        context = create_context(top_docs)\n",
        "        llm_query = system_prompt + \"Питання: \" + query + \"Контекст: \" + context + \" Відповідь: \"\n",
        "    else:\n",
        "        llm_query = system_prompt + \"Питання: \" + query +  \"Контекст: []\" + \" Відповідь: \"\n",
        "\n",
        "    client = Groq(api_key=groq_api_key)\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": llm_query,\n",
        "            }\n",
        "        ],\n",
        "        model=\"gemma-7b-it\",\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnMkySpQyYWc"
      },
      "source": [
        "# Web interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:49:15.305652Z",
          "iopub.status.busy": "2024-12-09T13:49:15.305270Z",
          "iopub.status.idle": "2024-12-09T13:49:15.321672Z",
          "shell.execute_reply": "2024-12-09T13:49:15.320717Z",
          "shell.execute_reply.started": "2024-12-09T13:49:15.305619Z"
        },
        "id": "bI4WO7Z_8iyH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_interface():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## Пошук документів і використання LLM для відповіді на питання про книжку з Історії України 7 клас\")\n",
        "        gr.Markdown(\"Цей сервіс дозволяє виконувати семантичний пошук, BM25 пошук, та реранкінг документів із можливістю використання контексту для запитів до LLM. \")\n",
        "        gr.Markdown(\"no-search означає, що для відповіді LLM буде використовувати свої власні знання. Якщо робити пошук, але не давати його в контекст, то LLM не має давати відповіді.\")\n",
        "        PDF_URL = \"https://files.pidruchnyk.com.ua/uploads/book/7-klas-istoriia-ukrainy-sorochynska-2020.pdf\"\n",
        "        gr.Markdown(f'<p style=\"font-size: 18px;\">Переглянути книгу можна за посиланням <a href=\"{PDF_URL}\" style=\"color: orange; text-decoration: underline;\">7 клас Історія України - Сорочинська (2020)</a></p>')\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                query_input = gr.Textbox(label=\"Запит\", placeholder=\"Наприклад, Що таке «Руська правда»?\")\n",
        "\n",
        "                method_input = gr.Radio(\n",
        "                    choices=[\"semantic\", \"bm25\", \"no search\"], label=\"Метод пошуку\", value=\"semantic\"\n",
        "                )\n",
        "\n",
        "                groq_key_input = gr.Textbox(label=\"API ключ Groq\", type=\"password\")\n",
        "\n",
        "                rerank_input = gr.Checkbox(label=\"Використовувати реранкер\", value=False)\n",
        "\n",
        "                temperature_input = gr.Slider(minimum=0, maximum=1, step=0.1, label=\"Температура\", value=0.5)\n",
        "                use_context = gr.Checkbox(label=\"Використовувати контекст\", value=True)\n",
        "\n",
        "\n",
        "\n",
        "            with gr.Column():\n",
        "                results_output = gr.Textbox(label=\"Результати пошуку та відповіді на питання\", interactive=False)\n",
        "\n",
        "                doc1_output = gr.Textbox(label=\"Документ 1\", interactive=False)\n",
        "                doc2_output = gr.Textbox(label=\"Документ 2\", interactive=False)\n",
        "                doc3_output = gr.Textbox(label=\"Документ 3\", interactive=False)\n",
        "\n",
        "        def choose_prompt(method):\n",
        "            if method == 'no search':\n",
        "                return \"\"\"\n",
        "                Ви спеціалізований агент, який відповідає на запитання. Відповідай ЛИШЕ українською мовою.\n",
        "                \"\"\"\n",
        "            else:\n",
        "                return \"\"\"\n",
        "                Ви спеціалізований агент, який відповідає на запитання, використовуючи тільки надані документи. Ваше завдання — вибрати найрелевантніші документи із доступних, які відповідають на запитання, і надати відповідь, базуючись виключно на них. Відповідь повинна містити посилання на документ з контексту, з якого була отримана інформація (наприклад, [1]). Якщо контекст порожній або всі документи не відповідають на запитання, ви НЕ надаєте відповідь. Якщо надано контекст, він буде вказаний після запитання в такому форматі: 'Питання: [запитання]. Контекст: [Документ 1. ... Документ 2. ...]'. ЗАВЖДИ використовуйте тільки інформацію з вибраних документів і НЕ використовуйте свої попередні знання, навіть якщо контексту немає. У разі відсутності контексту або якщо жоден документ не відповідає запитуваному, НЕ давайте відповіді. Надавайте відповідь лише українською мовою.\n",
        "                \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        def process_query(query, method, groq_api_key, rerank, temperature, use_context):\n",
        "            top_docs = perform_rag(query, index, points_sections, method=method, top_n=3, rerank=rerank)\n",
        "            system_prompt = choose_prompt(method)\n",
        "            answer = perform_llm(query, use_context, groq_api_key=groq_api_key, top_docs=top_docs, system_prompt=system_prompt, temperature=temperature)\n",
        "            docs_texts = [f\"Параграф {doc['paragraph_number']}, підпункт {doc['point_number']}. {doc['text']}\" for doc in top_docs]\n",
        "            while len(docs_texts) < 3:\n",
        "                docs_texts.append(\"\")\n",
        "            return answer, docs_texts[0], docs_texts[1], docs_texts[2]\n",
        "\n",
        "        gr.Button(\"Виконати пошук та отримати відповідь\").click(\n",
        "            process_query,\n",
        "            inputs=[query_input, method_input, groq_key_input, rerank_input, temperature_input, use_context],\n",
        "            outputs=[results_output, doc1_output, doc2_output, doc3_output]\n",
        "        )\n",
        "\n",
        "    return demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-09T13:49:17.888809Z",
          "iopub.status.busy": "2024-12-09T13:49:17.888441Z",
          "iopub.status.idle": "2024-12-09T13:49:19.209099Z",
          "shell.execute_reply": "2024-12-09T13:49:19.208354Z",
          "shell.execute_reply.started": "2024-12-09T13:49:17.888780Z"
        },
        "id": "9PaxWPgA9908",
        "outputId": "10a2af77-7033-462d-93a3-7d5d03737483",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7863\n",
            "Kaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "* Running on public URL: https://0472b8475a57dee290.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://0472b8475a57dee290.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "interface = create_interface()\n",
        "interface.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 6263522,
          "sourceId": 10146887,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30805,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
